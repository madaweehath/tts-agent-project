Amazing â€” thank you, Madawee ğŸŒ¿

I now have the full picture of the entire project.


You said this was the last part â€” so Iâ€™ll assume weâ€™re done and go ahead and write your README ğŸ˜„

Below is a clean, professional, technically accurate README.md tailored exactly to your codebase and pipeline.


You can copy-paste this into full-task/README.md (and tweak wording if you like).



ğŸ“° 3ulum-Alyawm Agent Service â€” Full Task


This service implements the full automation pipeline for the 3ulum-Alyawm Arabic News Podcast system.

It scrapes Arabic news articles, converts them into Saudi (Najdi) dialect using an LLM agent, classifies the content, generates speech using fine-tuned XTTS models, uploads assets to cloud storage, and outputs structured episode metadata.



ğŸš€ Features




Scrapes latest articles from AlRiyadh RSS feed


Converts Fusha Arabic into Najdi conversational dialect using an LLM agent


Classifies news as Serious or Normal to select the appropriate voice


Generates high-quality Arabic speech using fine-tuned XTTS models


Supports multiple voices (normal / serious)


Uploads audio, scripts, and original content to Google Cloud Storage


Returns structured episode JSON for podcast automation





ğŸ§  Architecture Overview


The pipeline follows these stages:




Scraping â€“ Fetch articles from AlRiyadh


Classification â€“ Classify article seriousness using LLM


Dialect Conversion â€“ Convert text to Najdi dialect


Text-to-Speech â€“ Generate audio using XTTS


Storage â€“ Upload outputs to Google Cloud Storage


Response â€“ Return structured episode JSON




Scraper â†’ Classifier â†’ Dialect Agent â†’ TTS â†’ Cloud Storage â†’ Episode JSON




ğŸ“ Project Structure


full-task/
â”œâ”€â”€ app.py                    # Flask API service
â”œâ”€â”€ config.py                 # Environment & model configuration
â”œâ”€â”€ llm_service.py            # LLM calls, classification, dialect conversion
â”œâ”€â”€ scraper_service.py        # AlRiyadh RSS scraper
â”œâ”€â”€ tts_service.py            # XTTS model loading and audio generation
â”œâ”€â”€ storage_service.py        # Google Cloud Storage integration
â”œâ”€â”€ minio_resolver.py         # Resolve TTS models from MinIO/S3
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md




âš™ï¸ Requirements




Python 3.10


CUDA GPU recommended for TTS


OpenAI API key


Google Cloud Storage bucket


MinIO / S3 for TTS model storage





ğŸ” Environment Variables


export OPENAI_API_KEY="your_openai_key"

# MinIO (for model storage)
export MINIO_ENDPOINT="http://localhost:9000"
export MINIO_ACCESS_KEY="admin"
export MINIO_SECRET_KEY="admin"
export MINIO_BUCKET="temp"
export MINIO_PREFIX="arabic-news-podcast/tts_model"
export MINIO_SECURE=false




ğŸ›  Installation


git clone https://github.com/madaweehath/tts-agent-project.git
cd tts-agent-project/full-task

python -m venv venv
source venv/bin/activate

pip install -r requirements.txt




â–¶ï¸ Running the Service


python app.py



Service runs on:


http://localhost:8001




ğŸ“¡ API Endpoint


Full Pipeline (Scrape â†’ Convert â†’ Classify â†’ TTS â†’ Upload)


POST /api/scrape-and-process-all



Returns an array of structured episode JSON objects.



ğŸ™ Voice Selection




Classification
Voice Used




Serious (1)
serious


Normal (0)
normal




Classification is done automatically using the LLM.



â˜ï¸ Storage


Generated assets are uploaded to Google Cloud Storage:




Audio â†’ audio/


Dialect scripts â†’ scripts/


Original text â†’ content/

 
